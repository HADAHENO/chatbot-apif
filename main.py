from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
import google.generativeai as genai

# ุฅุนุฏุงุฏ ุงูููุฏููุงุช
SentenceTransformer_model = SentenceTransformer("intfloat/multilingual-e5-large")

# Pinecone - ุฅูุดุงุก ูุงุฆู ุฌุฏูุฏ
pc = Pinecone(api_key="pcsk_3ax4D8_PH7vWF1KWAMRpyjmEnXhwxswmHSjvqgwovna3xGGbfsgZsMRtRyFi9uCpPyi4B9")
index = pc.Index("quickstart")

# Gemini
genai.configure(api_key="AIzaSyAThgYMlxd1FFg67pMnnBtxBbgDPk7qIkQ")
model = genai.GenerativeModel("gemini-2.0-flash")

# Flask app
app = Flask(__name__)
chat_history = []

# ุงูุจุญุซ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
def get_answer_from_pinecone(user_question):
    question_vector = SentenceTransformer_model.encode(user_question).tolist()
    search_result = index.query(vector=question_vector, top_k=3, include_metadata=True)
    best_match = max(search_result.matches, key=lambda x: x.score, default=None)

    if best_match and best_match.score > 0.7:
        return best_match.metadata.get('answer', 'ูุง ุชูุฌุฏ ุฅุฌุงุจุฉ ูุชููุฑุฉ')
    else:
        return "ูุง ุชูุฌุฏ ุฅุฌุงุจุฉ ุฏูููุฉ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช."

# ุงูุชูุงุตู ูุน Gemini
def ask_gemini_with_combined_answer(user_question, pinecone_answer, history=[]):
    context = "\n".join([f"๐ค ุงููุณุชุฎุฏู: {q}\n๐ค ุงูุฑุฏ: {a}" for q, a in history])
    prompt = f"""
โ ูุงู: ูุง ุชุณุชุฎุฏู ุฅูุง ุงููุนูููุงุช ุงูููุฌูุฏุฉ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช ููุท. ูุง ุชูุฎูููู.

๐ ุงููุญุงุฏุซุฉ ุงูุณุงุจูุฉ:
{context if context else 'ูุง ููุฌุฏ ุณูุงู ุณุงุจู.'}

๐ค ุงููุณุชุฎุฏู ูุณุฃู: {user_question}
๐ ุงููุนูููุฉ ุงููุณุชุฎุฑุฌุฉ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช: "{pinecone_answer or 'ูุง ุชูุฌุฏ'}"

โ ุงููุทููุจ: 
- ุฅุฐุง ูุงู ุงูุณุคุงู ุงูุญุงูู ูุฑุชุจุทูุง ุจุงููุญุงุฏุซุฉ ุงูุณุงุจูุฉุ ุงุณุชุฎุฏู ุงูุณูุงู ูุงูุฅุฌุงุจุฉ ุงูููุชุจุณุฉ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุนูุง.
- ุฅุฐุง ูู ููู ูุฑุชุจุทูุงุ ุชุฌุงูู ุงูุณูุงู ุงูุณุงุจู ุชูุงููุง.
- โ ูุง ุชุถู ุฃู ูุนูููุฉ ูู ุฎุงุฑุฌ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุญุชู ูู ููุช ุชุนุฑููุง.
- โ ุฃุฌุจ ุจุฌููุฉ ูุงุญุฏุฉ ูุงุถุญุฉ.

๐ ุงูุฑุฏ:
"""
    response = model.generate_content(prompt)
    return response.text.strip()

# Endpoint
@app.route("/ask", methods=["POST"])
def ask_question():
    data = request.json
    user_q = data.get("question", "")
    if not user_q:
        return jsonify({"error": "ูู ูุชู ุชูููุฑ ุณุคุงู"}), 400

    pinecone_answer = get_answer_from_pinecone(user_q)
    final_answer = ask_gemini_with_combined_answer(user_q, pinecone_answer, chat_history)
    chat_history.append((user_q, final_answer))

    return jsonify({"answer": final_answer})

# ููุญูุธุฉ: ูุง ูุณุชุฎุฏู app.run() ูู Hugging Face Spaces
